# Design Decisions & Trade-offs

This document captures key architectural and design decisions made during development.

---

## 1. Dual Scoring System (Algo + AI)

**Decision:** Use both algorithmic scoring (0-100) and AI evaluation (0-100), averaged for final score.

**Trade-offs:**
| Approach | Pros | Cons |
|----------|------|------|
| Algo only | Instant, deterministic, cheap | Misses nuanced signals |
| AI only | Rich reasoning, handles edge cases | Slow, expensive, non-deterministic |
| **Dual (chosen)** | Best of both, transparent breakdown | More complex, 2x scoring logic |

**Rationale:** Algo catches objective criteria (years exp, ACV), AI catches subjective signals (culture fit, red flags). Simple average = transparent, no "black box" weighting.

---

## 2. Model Selection

**Decision:** 
- Extraction: `google/gemini-2.5-flash` 
- Scoring: `openai/gpt-5-mini`

**Trade-offs:**
| Model | Speed | Cost | Quality |
|-------|-------|------|---------|
| Gemini Flash | âš¡ Very fast (~1s) | ğŸ’° Very cheap | âœ… Good for extraction |
| GPT-5-mini | ğŸ¢ Slower (~10s) | ğŸ’°ğŸ’° Moderate | âœ… Better reasoning |
| GPT-4o | ğŸ¢ğŸ¢ Slowest | ğŸ’°ğŸ’°ğŸ’° Expensive | âœ…âœ… Best quality |

**Rationale:** Extraction is structured data parsing (Flash sufficient). Scoring requires reasoning about candidate fit (GPT-5-mini worthwhile).

---

## 3. Streaming Pipeline Architecture

**Decision:** Process in small batches (5), stream results as they complete.

**Trade-offs:**
| Approach | First Result | Total Time | Complexity |
|----------|--------------|------------|------------|
| Sequential (all extract â†’ all score) | ~70s | ~70s | Simple |
| **Streaming batches (chosen)** | ~10s | ~50s | Moderate |
| Fully parallel (all at once) | ~15s | ~15s | High + rate limits |

**Rationale:** Streaming provides best UX (early results) without hitting API rate limits. Batch size 5 balances parallelism vs. API quotas.

---

## 4. Progressive Loading UX

**Decision:** Show extracted data immediately while scoring runs in background.

**Implementation:**
1. Upload â†’ Extraction (~12s) â†’ **Show preview cards**
2. Scoring runs in background â†’ Update cards with scores
3. Interview questions generated **on-demand** (lazy loading)

**Trade-offs:**
- âœ… User sees meaningful data in ~12s instead of 70s
- âœ… Perceived performance vastly improved
- âŒ Slightly more complex frontend state management

---

## 5. Interview Questions: Eager vs Lazy

**Decision:** Generate interview questions during scoring (eager), but could switch to on-demand.

**Trade-offs:**
| Approach | Speed | UX | API Calls |
|----------|-------|-----|-----------|
| Eager (current) | Slower initial load | Questions ready when needed | 2x per candidate |
| **Lazy (recommended)** | Faster initial load | Brief delay on click | 1 per candidate clicked |

**Future optimization:** Switch to lazy loading to reduce initial scoring time by ~50%.

---

## 6. Pydantic for All LLM Outputs

**Decision:** Strict Pydantic models for all LLM responses.

**Trade-offs:**
- âœ… Type safety, validation, clear contracts
- âœ… Catches malformed responses immediately
- âœ… Self-documenting API
- âŒ Slightly more boilerplate code

**Rationale:** LLM outputs are unpredictable. Pydantic catches errors before they propagate downstream.

---

## 7. Data Preservation Strategy

**Decision:** Preserve all original CSV columns, fill missing values in-place, add new extracted columns.

**Trade-offs:**
- âœ… Original data untouched (can always reference)
- âœ… No data loss
- âŒ Larger output files

**Implementation:** 
- Original column empty? â†’ Fill with extracted value
- Original has data? â†’ Keep original
- New semantic fields â†’ Add as new columns

---

## 8. Red Flag Detection

**Decision:** Integrate red flag detection into extraction phase, not as separate pass.

**Trade-offs:**
- âœ… Single LLM call handles both extraction + red flags
- âœ… Context available for better red flag detection
- âŒ Slightly larger prompt

**Red flags detected:**
- Job hopping (avg tenure < 18 months)
- Title inflation
- Employment gaps > 6 months
- Overqualified (expects > $200k OTE)

---

## 9. Bio Summary in First Person

**Decision:** Generate bio summaries in first person ("I am...", "I have...").

**Rationale:** Reads more naturally in candidate profiles, feels more personal, better for potential candidate-facing applications.

---

## 10. Batch Size: 5 vs 10 vs 20

**Decision:** Batch size of 5 for optimal streaming UX.

**Analysis:**
| Batch Size | Batches for 21 candidates | First visible | Rate limit risk |
|------------|---------------------------|---------------|-----------------|
| 5 | 5 batches | ~5s | Low |
| 10 | 3 batches | ~8s | Medium |
| 20 | 2 batches | ~15s | High |

**Rationale:** Smaller batches = faster first results = better perceived performance.

---

## 11. API Provider: OpenRouter

**Decision:** Use OpenRouter as unified API gateway for multiple LLM providers.

**Trade-offs:**
- âœ… Single API key for Gemini, OpenAI, Anthropic, etc.
- âœ… Easy model switching without code changes
- âœ… Built-in fallback support
- âŒ Slight latency overhead (~50ms)
- âŒ Dependency on third-party service

---

## 12. Frontend Stack

**Decision:** Next.js 16 + Tailwind CSS + Custom glassmorphism theme.

**Trade-offs:**
- âœ… Modern React with App Router
- âœ… Fast development with Tailwind
- âœ… Premium dark theme with animations
- âŒ Tailwind v4 has some breaking changes from v3

---

## Summary

The key principle throughout: **optimize for user experience first**, then performance, then simplicity. Stream results early, show progress, keep the user engaged with tips while they wait.
